{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a6d6a9f-7040-4fb9-9704-afb356c6c8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All required artifacts are found. Notebook is ready\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "sys.path.insert(0, str(PROJECT_ROOT/\"src\"))\n",
    "\n",
    "# Startup checks\n",
    "from startup_checks import ensure_project_dirs, check_required_artifacts\n",
    "\n",
    "ensure_project_dirs()\n",
    "# tokenization_config.json, comparison artifacts and BERT/FinBERT artifacts are required\n",
    "check_required_artifacts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3e4dbd7-e2b1-467f-a15b-41554e381f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import BERT_KEY, FINBERT_KEY, HF_MODELS\n",
    "from analysis import load_model_artifacts, build_metrics_comparison_table, get_primary_score, get_confusion_matrix, top_confusions, get_split_scores\n",
    "from artifacts_utils import load_tokenization_config, save_final_model_selection\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5eaea6d-4249-474e-8fd7-36ab6c9205c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization config:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'baseline_max_length': 96,\n",
       " 'max_length': 96,\n",
       " 'padding': 'max_length',\n",
       " 'truncation': True,\n",
       " 'selection_method': 'truncation_efficiency_tradeoff',\n",
       " 'truncation_threshold_pct': 1.0}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load artifacts for BERT and FinBert\n",
    "bert_art = load_model_artifacts(BERT_KEY)\n",
    "finbert_art = load_model_artifacts(FINBERT_KEY)\n",
    "\n",
    "token_cfg = load_tokenization_config()\n",
    "\n",
    "print(\"Tokenization config:\")\n",
    "display(token_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5425d62a-d096-4af6-b80f-833f096e516a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>split</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_accuracy</th>\n",
       "      <th>eval_macro_f1</th>\n",
       "      <th>eval_macro_precision</th>\n",
       "      <th>eval_macro_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bert-base</td>\n",
       "      <td>val</td>\n",
       "      <td>0.48184</td>\n",
       "      <td>0.796233</td>\n",
       "      <td>0.741575</td>\n",
       "      <td>0.735577</td>\n",
       "      <td>0.749504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>finbert</td>\n",
       "      <td>val</td>\n",
       "      <td>0.42835</td>\n",
       "      <td>0.806507</td>\n",
       "      <td>0.775123</td>\n",
       "      <td>0.761411</td>\n",
       "      <td>0.811377</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       model split  eval_loss  eval_accuracy  eval_macro_f1  \\\n",
       "0  bert-base   val    0.48184       0.796233       0.741575   \n",
       "1    finbert   val    0.42835       0.806507       0.775123   \n",
       "\n",
       "   eval_macro_precision  eval_macro_recall  \n",
       "0              0.735577           0.749504  \n",
       "1              0.761411           0.811377  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>split</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_accuracy</th>\n",
       "      <th>eval_macro_f1</th>\n",
       "      <th>eval_macro_precision</th>\n",
       "      <th>eval_macro_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bert-base</td>\n",
       "      <td>test</td>\n",
       "      <td>0.438863</td>\n",
       "      <td>0.805128</td>\n",
       "      <td>0.752194</td>\n",
       "      <td>0.744269</td>\n",
       "      <td>0.763485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>finbert</td>\n",
       "      <td>test</td>\n",
       "      <td>0.419923</td>\n",
       "      <td>0.803419</td>\n",
       "      <td>0.767500</td>\n",
       "      <td>0.755469</td>\n",
       "      <td>0.801047</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       model split  eval_loss  eval_accuracy  eval_macro_f1  \\\n",
       "0  bert-base  test   0.438863       0.805128       0.752194   \n",
       "1    finbert  test   0.419923       0.803419       0.767500   \n",
       "\n",
       "   eval_macro_precision  eval_macro_recall  \n",
       "0              0.744269           0.763485  \n",
       "1              0.755469           0.801047  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build a comparison table for metrics\n",
    "model_artifacts_map = {\n",
    "    BERT_KEY: bert_art,\n",
    "    FINBERT_KEY: finbert_art\n",
    "}\n",
    "\n",
    "metric_keys = [\"eval_loss\", \"eval_accuracy\", \"eval_macro_f1\", \"eval_macro_precision\", \"eval_macro_recall\"]\n",
    "\n",
    "df_val_compare = build_metrics_comparison_table(\n",
    "    model_artifacts_map=model_artifacts_map,\n",
    "    split=\"val\",\n",
    "    metric_keys=metric_keys\n",
    ")\n",
    "\n",
    "df_test_compare = build_metrics_comparison_table(\n",
    "    model_artifacts_map=model_artifacts_map,\n",
    "    split=\"test\",\n",
    "    metric_keys=metric_keys\n",
    ")\n",
    "\n",
    "display(df_val_compare, df_test_compare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bb021d4-5a08-48c4-85e3-93530e83a798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT val macro_f1: 0.7415754421031572\n",
      "FinBERT val macro_f1: 0.7751226453612198\n",
      "Selected model (by val): finbert\n",
      "\n",
      "Test macro_f1 (report only):\n",
      "BERT test macro_f1: 0.7521942203266261\n",
      "FinBERT test macro_f1: 0.7675000443474712\n",
      "\n",
      "Delta (BERT - FinBERT) val macro_f1: -0.0335472032580626\n"
     ]
    }
   ],
   "source": [
    "# Choose winner based on macro F1-score\n",
    "bert_val_f1 = get_primary_score(bert_art, \"val\")\n",
    "finbert_val_f1 = get_primary_score(finbert_art, \"val\")\n",
    "\n",
    "winner = BERT_KEY if bert_val_f1 >= finbert_val_f1 else FINBERT_KEY\n",
    "print(\"BERT val macro_f1:\", bert_val_f1)\n",
    "print(\"FinBERT val macro_f1:\", finbert_val_f1)\n",
    "print(\"Selected model (by val):\", winner)\n",
    "\n",
    "print(\"\\nTest macro_f1 (report only):\")\n",
    "print(\"BERT test macro_f1:\", get_primary_score(bert_art, \"test\"))\n",
    "print(\"FinBERT test macro_f1:\", get_primary_score(finbert_art, \"test\"))\n",
    "\n",
    "delta = bert_val_f1 - finbert_val_f1\n",
    "print(\"\\nDelta (BERT - FinBERT) val macro_f1:\", delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2429a077-0060-4715-af1f-8bd76b62afb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label order (BERT):    ['negative', 'neutral', 'positive']\n",
      "Label order (FinBERT): ['negative', 'neutral', 'positive']\n",
      "\n",
      "CM normalize (BERT): true\n",
      "CM normalize (FinBERT): true\n",
      "\n",
      "Confusion matrix (BERT):\n",
      " [[0.59302326 0.19767442 0.20930233]\n",
      " [0.1086262  0.82108626 0.07028754]\n",
      " [0.05913978 0.06451613 0.87634409]]\n",
      "Confusion matrix (FinBERT):\n",
      " [[0.75581395 0.05813953 0.18604651]\n",
      " [0.17252396 0.77635783 0.05111821]\n",
      " [0.05376344 0.07526882 0.87096774]]\n"
     ]
    }
   ],
   "source": [
    "# Sanity check for saved evaluation artifacts\n",
    "bert_label_order = bert_art[\"label_map\"][\"label_order\"]\n",
    "finbert_label_order = finbert_art[\"label_map\"][\"label_order\"]\n",
    "\n",
    "print(\"Label order (BERT):   \", bert_label_order)\n",
    "print(\"Label order (FinBERT):\", finbert_label_order)\n",
    "\n",
    "assert bert_label_order == finbert_label_order, \"Label order mismatch between models\"\n",
    "\n",
    "bert_num_labels = len(bert_label_order)\n",
    "finbert_num_labels = len(finbert_label_order)\n",
    "\n",
    "bert_cm = get_confusion_matrix(bert_art, \"test\")\n",
    "finbert_cm = get_confusion_matrix(finbert_art, \"test\")\n",
    "\n",
    "bert_cm_normalize = bert_art[\"evaluation\"][\"test\"][\"metadata\"][\"confusion_matrix_normalize\"]\n",
    "finbert_cm_normalize = finbert_art[\"evaluation\"][\"test\"][\"metadata\"][\"confusion_matrix_normalize\"]\n",
    "\n",
    "print(\"\\nCM normalize (BERT):\", bert_cm_normalize)\n",
    "print(\"CM normalize (FinBERT):\", finbert_cm_normalize)\n",
    "\n",
    "assert bert_cm_normalize == finbert_cm_normalize, \"CM normalize mode mismatch between models\"\n",
    "\n",
    "print(\"\\nConfusion matrix (BERT):\\n\", bert_cm)\n",
    "print(\"Confusion matrix (FinBERT):\\n\", finbert_cm)\n",
    "\n",
    "assert bert_cm.shape == (bert_num_labels, bert_num_labels), f\"Unexpected BERT CM shape: {bert_cm.shape}\"\n",
    "assert finbert_cm.shape == (finbert_num_labels, finbert_num_labels), f\"Unexpected FinBERT CM shape: {finbert_cm.shape}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95686320-9510-426f-adbd-51bdb4d727ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT top confusions\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_true_label</th>\n",
       "      <th>y_pred_label</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  y_true_label y_pred_label  count\n",
       "0      neutral     negative     34\n",
       "1      neutral     positive     22\n",
       "2     negative     positive     18\n",
       "3     negative      neutral     17\n",
       "4     positive      neutral     12\n",
       "5     positive     negative     11"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FinBERT top confusions\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_true_label</th>\n",
       "      <th>y_pred_label</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  y_true_label y_pred_label  count\n",
       "0      neutral     negative     54\n",
       "1     negative     positive     16\n",
       "2      neutral     positive     16\n",
       "3     positive      neutral     14\n",
       "4     positive     negative     10\n",
       "5     negative      neutral      5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Error analysis\n",
    "bert_top = top_confusions(bert_art[\"predictions\"][\"test\"])\n",
    "finbert_top = top_confusions(finbert_art[\"predictions\"][\"test\"])\n",
    "\n",
    "print(\"BERT top confusions\")\n",
    "display(bert_top)\n",
    "    \n",
    "print(\"\\nFinBERT top confusions\")\n",
    "display(finbert_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ccb5ea1-6a0c-4a1e-842c-6488a13bad53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Model Selection Summary ===\n",
      "Selection rule: choose by validation macro_f1 (to avoid test leakage)\n",
      "Winner: finbert\n",
      "\n",
      "validation macro_f1:\n",
      "- BERT:    0.7416\n",
      "- FinBERT: 0.7751\n",
      "- Delta (BERT - FinBERT): -0.0335\n",
      "\n",
      "test macro_f1 (report only):\n",
      "- BERT:    0.7522\n",
      "- FinBERT: 0.7675\n"
     ]
    }
   ],
   "source": [
    "# Decision summary\n",
    "bert_test_f1 = get_primary_score(bert_art, \"test\")\n",
    "finbert_test_f1 = get_primary_score(finbert_art, \"test\")\n",
    "\n",
    "print(\"=== Model Selection Summary ===\")\n",
    "print(f\"Selection rule: choose by validation macro_f1 (to avoid test leakage)\")\n",
    "print(f\"Winner: {winner}\")\n",
    "\n",
    "print(\"\\nvalidation macro_f1:\")\n",
    "print(f\"- BERT:    {bert_val_f1:.4f}\")\n",
    "print(f\"- FinBERT: {finbert_val_f1:.4f}\")\n",
    "print(f\"- Delta (BERT - FinBERT): {(delta):.4f}\")\n",
    "\n",
    "print(\"\\ntest macro_f1 (report only):\")\n",
    "print(f\"- BERT:    {bert_test_f1:.4f}\")\n",
    "print(f\"- FinBERT: {finbert_test_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11ce509a-6dc2-4654-ae86-90da19562a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to /home/tl/stock-news-sentiment-bert-finbert/artifacts/results/comparison/final_model_selection.json\n",
      "Saved final model selection to: /home/tl/stock-news-sentiment-bert-finbert/artifacts/results/comparison/final_model_selection.json\n"
     ]
    }
   ],
   "source": [
    "# Save final model selection\n",
    "selection = {\n",
    "    \"selected_model\": {\n",
    "        \"model_key\": winner,\n",
    "        \"model_id\": HF_MODELS[winner][\"model_id\"]\n",
    "    },\n",
    "    \"selection_criteria\": {\n",
    "        \"primary_metric\": \"macro_f1\",\n",
    "        \"split\": \"val\",\n",
    "        \"decision_rule\": \"max\",\n",
    "        \"implicit_decision_rule\": {\n",
    "            \"type\": \"prefer_model\",\n",
    "            \"model_key\": \"bert-base\",\n",
    "            \"when\": \"score_tie\"            \n",
    "        }\n",
    "    },\n",
    "    \"scores\": {\n",
    "        BERT_KEY: get_split_scores(bert_art),\n",
    "        FINBERT_KEY: get_split_scores(finbert_art)\n",
    "    },\n",
    "    \"environment\": {\n",
    "        \"tokenization_config\": load_tokenization_config(),\n",
    "        BERT_KEY: bert_art[\"label_map\"],\n",
    "        FINBERT_KEY: finbert_art[\"label_map\"],\n",
    "    },\n",
    "    \"metadata\": {\n",
    "        \"created_at\": datetime.now().isoformat(),\n",
    "        \"pipeline_stage\": \"model_selection\"\n",
    "    },\n",
    "    \"notes\": \"Final model selected based on validation macro F1-score\"\n",
    "}\n",
    "\n",
    "final_model_selection_path = save_final_model_selection(selection)\n",
    "print(\"Saved final model selection to:\", final_model_selection_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3b3f47-3c9d-45a8-988c-79d8ff2b385b",
   "metadata": {},
   "source": [
    "## Summary - Model Comparison and Selection (BERT vs FinBERT)\n",
    "\n",
    "### Objective\n",
    "To compare the Transformer models (BERT vs FinBERT) using a consistent evaluation framework, select a final model based on validation performance, and perform lightweight error analysis to support the decision.\n",
    "\n",
    "### Work Performed\n",
    "- Built side-by-side comparison tables for key validation and test metrics\n",
    "- Selected a “winner” model using **validation macro F1** as the primary decision score (test set used for reporting only)\n",
    "- Performed sanity checks on saved evaluation artifacts (label order alignment, confusion matrix shape, normalization mode)\n",
    "- Conducted error analysis by extracting the most frequent misclassification pairs from test predictions\n",
    "\n",
    "### Key Decisions\n",
    "- Chose the final model using validation macro F1 to avoid test leakage\n",
    "- Standardized comparison inputs by reusing the same tokenization config and artifact-loading mechanisms for both models\n",
    "- Used confusion matrices and “top confusions” as interpretability checks, not as primary selection criteria\n",
    "\n",
    "### Results\n",
    "- Identified a selected model (“winner”) based on validation macro F1-score\n",
    "- Produced interpretable error patterns (most common label confusions) to highlight model weaknesses\n",
    "\n",
    "### Artifacts Produced\n",
    "- Final model selection:\n",
    "  - **artifacts/result/comparison/final_model_selection.json**\n",
    "\n",
    "### Artifacts Used\n",
    "- BERT artifacts:\n",
    "  - **artifacts/results/bert-base/**\n",
    "  - **artifacts/models/best/bert-base/**\n",
    "- FinBERT artifacts:\n",
    "  - **artifacts/results/finbert/**\n",
    "  - **artifacts/models/best/finbert/**\n",
    "- Shared preprocessing config:\n",
    "  - **artifacts/preprocessing/tokenization_config.json**\n",
    "- Split artifacts (per model):\n",
    "  - Label map: **label_map.json**\n",
    "  - Best model info: **best_model_info.json**\n",
    "  - Metrics: **val_metrics.json**, **test_metrics.json**\n",
    "  - Structured evaluation: **val_evaluation.json**, **test_evaluation.json**\n",
    "  - Predictions: **val_prediction.csv**, **test_prediction.csv**\n",
    "  - Training log: **training_log_history.csv**\n",
    "\n",
    "### Takeaway\n",
    "This notebook completes the model comparison and selection stage by choosing the final model using validation macro F1-score, supported by artifact sanity checks and lightweight error analysis for interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f60deba-8318-40c3-a35d-4dbd4ef5faea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
